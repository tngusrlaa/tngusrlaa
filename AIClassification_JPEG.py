import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import seaborn as sns
from scipy.stats import ks_2samp, anderson_ksamp, chisquare
from sklearn.model_selection import learning_curve  # ì˜¬ë°”ë¥¸ ì„í¬íŠ¸ ìœ„ì¹˜
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
import tempfile, os
import warnings
import matplotlib
matplotlib.use('Qt5Agg')  # ë˜ëŠ” 'Qt5Agg'ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
import matplotlib.pyplot as plt
import shutil
import gc

warnings.filterwarnings("ignore")

# ì„ì‹œ í´ë” ì§€ì • (ë©”ëª¨ë¦¬ë§µ ì˜¤ë¥˜ ë°©ì§€)
os.environ['JOBLIB_TEMP_FOLDER'] = tempfile.mkdtemp()

# ë°ì´í„° ì „ì²˜ë¦¬
file_path = "C:/Users/KIM/AppData/Local/JetBrains/PyCharm2024.1/cpython-cache/Users/KIM/PycharmProjects/250409_jpgstructralanly/250428/250517_1024ìµœì¢…ë°ì´í„°ì…‹.csv"
df = pd.read_csv(file_path)


# 1. 'Subfolder Name'ì´ 'n'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë°ì´í„° ì œì™¸
df = df[~df['Subfolder Name'].str.startswith('n')]  # 'n'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë°ì´í„° ì œê±°

# 2. 'Subfolder Name'ì„ ê¸°ì¤€ìœ¼ë¡œ Label Encoding ìˆ˜í–‰
le = LabelEncoder()
df['AI_service_label'] = le.fit_transform(df['Subfolder Name'].astype(str))  # 'Subfolder Name'ì„ Label Encoding

# ì¶œë ¥: Subfolder Nameê³¼ AI_service_label ë§¤í•‘
subfolder_to_label = dict(zip(le.classes_, range(len(le.classes_))))

# 3. íŠ¹ì„± ë°ì´í„°ì™€ ë ˆì´ë¸” ë°ì´í„° ë¶„ë¦¬
X = df.drop(columns=['AI_service_label', 'Subfolder Name', 'File Name'])  # 'AI_service_label', 'Subfolder Name', 'File Name' ì œì™¸
y = df['AI_service_label']  # 'Subfolder Name'ì— ëŒ€í•œ Label Encoding ê°’ (AI ì„œë¹„ìŠ¤ êµ¬ë¶„)

# 4. '_seq'ì™€ '_info'ë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ ì œì™¸
cols_to_drop = [col for col in X.columns if col.endswith('_seq') or col.endswith('_info')]
X = X.drop(columns=cols_to_drop)  # í•´ë‹¹ ì»¬ëŸ¼ë“¤ì„ ì œê±°
print(f"ì œì™¸ëœ ì»¬ëŸ¼ë“¤: {cols_to_drop}")


# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
def handle_missing_data(X):
    missing_ratios = X.isnull().mean()  # ê° ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
    cols_to_drop = missing_ratios[missing_ratios >= 0.5].index  # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 50% ì´ìƒì¸ ì»¬ëŸ¼ë“¤
    X = X.drop(columns=cols_to_drop)  # í•´ë‹¹ ì»¬ëŸ¼ë“¤ ì œê±°
    print(f"ì œê±°ëœ ì»¬ëŸ¼ë“¤ (ê²°ì¸¡ì¹˜ ë¹„ìœ¨ 50% ì´ìƒ): {cols_to_drop.tolist()}")

    cols_to_fill = missing_ratios[missing_ratios < 0.5].index  # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 50% ë¯¸ë§Œì¸ ì»¬ëŸ¼ë“¤
    X[cols_to_fill] = X[cols_to_fill].fillna('Missing')  # ê²°ì¸¡ì¹˜ëŠ” 'Missing'ìœ¼ë¡œ ì²˜ë¦¬
    return X

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì ìš©
X = handle_missing_data(X)

# ë ˆì´ë¸” ì¸ì½”ë”©
cols_to_encode = X.select_dtypes(include=['object']).columns.tolist()
for col in cols_to_encode:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    print(f"Label Encoding for column '{col}': {le.classes_}")

# íŠ¹ì„± ì´ë¦„ì„ ì¼ê´€ë˜ê²Œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
def clean_feature_names(columns):
    return [col.replace(" ", "_").replace("<", "").replace(">", "").replace(",", "").replace("[", "_").replace("]", "") for col in columns]

# ëª¨ë¸ ì •ì˜ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
models = {
    "Logistic Regression": {
        "model": LogisticRegression(max_iter=10000, solver='saga', multi_class='multinomial'),
        "params": {
            'C': [0.1, 1, 10],
            'penalty': ['l2', 'l1'],
            'solver': ['liblinear', 'saga']
        }
    },
    "Random Forest": {
        "model": RandomForestClassifier(random_state=42),
        "params": {
            'n_estimators': [50, 100],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2],
            'class_weight': ['balanced', None]
        }
    },
    "XGBoost": {
        "model": XGBClassifier(eval_metric='logloss'),
        "params": {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 6, 10],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
    },
    "Decision Tree": {
        "model": DecisionTreeClassifier(random_state=42),
        "params": {
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
    },
    "SVM": {
        "model": SVC(probability=True),
        "params": {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']
        }
    },
    "KNN": {
        "model": KNeighborsClassifier(),
        "params": {
            'n_neighbors': [3, 5, 7, 9],
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
        }
    },
}

model_results = {}

# ë°ì´í„° ë¶„í• 
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)
print(f"\nTrain size: {X_train.shape[0]}, Validation size: {X_val.shape[0]}, Test size: {X_test.shape[0]}")

# ëª¨ë¸ íŠœë‹ ë° í‰ê°€ í•¨ìˆ˜
def tune_and_evaluate_model(name, model_info):
    print(f"\nğŸš€ ëª¨ë¸: {name}")
    model = model_info["model"]
    params = model_info["params"]

    # íŠ¹ì„± ì´ë¦„ì„ ì¼ê´€ë˜ê²Œ ì •ë¦¬
    X_train.columns = clean_feature_names(X_train.columns)
    X_val.columns = clean_feature_names(X_val.columns)
    X_test.columns = clean_feature_names(X_test.columns)

    grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='accuracy', n_jobs=1, verbose=1)
    grid_search.fit(X_train, y_train)

    print(f" - ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
    print(f" - ìµœì  êµì°¨ê²€ì¦ ì •í™•ë„: {grid_search.best_score_:.4f}")

    best_model = grid_search.best_estimator_
    y_val_pred = best_model.predict(X_val)
    val_acc = accuracy_score(y_val, y_val_pred)
    print(f" - ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
    print(classification_report(y_val, y_val_pred))

    y_test_pred = best_model.predict(X_test)
    test_acc = accuracy_score(y_test, y_test_pred)
    print(f" - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
    print(classification_report(y_test, y_test_pred))

    print(" - ğŸ” í•´ì„ ì •ë³´: ")
    try:
        if hasattr(best_model, "coef_"):
            coef_df = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': best_model.coef_[0]})
            print(coef_df.sort_values(by='Coefficient', key=abs, ascending=False).head(10))
        elif hasattr(best_model, "feature_importances_"):
            fi_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': best_model.feature_importances_})
            print(fi_df.sort_values(by='Importance', ascending=False).head(10))
        else:
            print("  > SHAP ì‚¬ìš© ì‹œë„ ì¤‘...")  # SHAP ìƒëµ
    except Exception as e:
        print(f"  > SHAP í•´ì„ ì˜ˆì™¸ ë°œìƒ: {e}")

    model_results[name] = {
        "Best Params": grid_search.best_params_,
        "CV Mean": grid_search.best_score_,
        "Validation Accuracy": val_acc,
        "Test Accuracy": test_acc
    }

    # ëª¨ë¸ í•™ìŠµ í›„ ë¶ˆí•„ìš”í•œ ê°ì²´ë“¤ ë©”ëª¨ë¦¬ í•´ì œ
    del grid_search
    gc.collect()  # ë¶ˆí•„ìš”í•œ ê°ì²´ë“¤ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ

    if name == "Random Forest":
        global final_model, y_pred_ai
        final_model = best_model
        y_pred_ai = y_test_pred

    train_class_counts = y_train.value_counts().sort_index()
    test_class_counts = y_test.value_counts().sort_index()
    expected = (test_class_counts.sum() * train_class_counts) / train_class_counts.sum()
    chi2_stat, p_val = chisquare(test_class_counts, expected)
    print(f"\nChi-square test: Stat={chi2_stat}, P={p_val}")

    # KS ê²€ì •
    train_probs = best_model.predict_proba(X_train)[:, 1]
    test_probs = best_model.predict_proba(X_test)[:, 1]
    ks_stat, ks_p_val = ks_2samp(train_probs, test_probs)
    print(f"KS ê²€ì •: í†µê³„ëŸ‰={ks_stat:.4f}, p-value={ks_p_val:.4f}")

    # AD ê²€ì •
    ad_stat, _, sig_level = anderson_ksamp([train_probs, test_probs])
    print(f"Anderson-Darling ê²€ì •: í†µê³„ëŸ‰={ad_stat:.4f}, ìœ ì˜ìˆ˜ì¤€={sig_level}")

    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_test, y_test_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues") #, xticklabels=[], yticklabels=[])
    plt.xlabel("Predicted label")
    plt.ylabel("Actual label")
    plt.title(f"Confusion Matrix: {name}")
    plt.show()

    # í•™ìŠµ ì „ ë©”ëª¨ë¦¬ í•´ì œ ì‹œë„
    gc.collect()

    #print(f"X_train ë°ì´í„° íƒ€ì…: {type(X_train)}")
    #print(f"y_train ë°ì´í„° íƒ€ì…: {type(y_train)}")

    # pandas DataFrameì„ numpy ë°°ì—´ë¡œ ë³€í™˜
    #X_train_np = X_train.to_numpy().astype(np.float32)  # íƒ€ì…ì„ ëª…í™•í•˜ê²Œ ì§€ì •
    #y_train_np = y_train.to_numpy().astype(np.int32)  # y ê°’ì€ int32ë¡œ ëª…í™•íˆ ì§€ì •

    # í•™ìŠµ ê³¡ì„ 
    train_sizes, train_scores, val_scores = learning_curve(best_model, X_train, y_train, cv=5, n_jobs=1)
    plt.figure(figsize=(8, 6))
    plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')
    plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation score')
    plt.title(f"Learning Curve for {name}")
    plt.xlabel('Training Size')
    plt.ylabel('Score')
    plt.legend()
    plt.show()

    # ëª¨ë¸ í•™ìŠµ í›„ ë¶ˆí•„ìš”í•œ ê°ì²´ë“¤ ë©”ëª¨ë¦¬ í•´ì œ
    del best_model
    gc.collect()  # ë¶ˆí•„ìš”í•œ ê°ì²´ë“¤ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ

    # ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì¶œë ¥
    misclassified = pd.DataFrame({
        'Subfolder Name': df.loc[y_test.index, 'Subfolder Name'],
        'File Name': df.loc[y_test.index, 'File Name'],
        'Actual': y_test,
        'Predicted': y_test_pred
    })
    misclassified = misclassified[misclassified['Actual'] != misclassified['Predicted']]
    print(f"\nì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ({name}):")
    print(misclassified[['Subfolder Name', 'File Name', 'Actual', 'Predicted']])

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for name, model_info in models.items():
    tune_and_evaluate_model(name, model_info)

# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ì¶œë ¥
results_df = pd.DataFrame(model_results).T.sort_values(by="Validation Accuracy", ascending=False)
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ:")
print(results_df)
best_model_name = results_df.index[0]
print(f"\nâœ… ìµœì  ëª¨ë¸: {best_model_name}")

# ì‹œê°í™” ë° í†µê³„ í…ŒìŠ¤íŠ¸
for label, data in zip(["Train", "Validation", "Test"], [y_train, y_val, y_test]):
    plt.figure(figsize=(8, 4))
    sns.countplot(x=data, hue=data, palette="Set2", legend=False)
    plt.title(f"{label} Dataset Distribution")
    plt.xlabel("Class (0: AI Image, 1: Human Image)")
    plt.ylabel("Count")
    plt.show()

# ì „ì²´ ì½”ë“œ ëë‚˜ê³ 
gc.collect()  # ì „ì²´ ì‘ì—… ëë‚œ í›„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰

# ì„ì‹œ í´ë” ì‚­ì œ
shutil.rmtree(os.environ['JOBLIB_TEMP_FOLDER'])  # ì„ì‹œ í´ë” ì‚­ì œ
